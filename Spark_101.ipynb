{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark_101.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPUxBQ4nar3sYIMswMFV3qR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akash865/Watchdogs/blob/master/Spark_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lihDEjAhyqca"
      },
      "source": [
        "# Spark 101 - Getting started with spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9t08dFEeA9W"
      },
      "source": [
        "## **Running Spark in Colab**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0qXKDZMzLDc"
      },
      "source": [
        "Running spark codes need some library imports. Please follow notebook to get started."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8dsXbdzzHDc"
      },
      "source": [
        "### Initialize Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1UN3lvn1sKf"
      },
      "source": [
        "The below line of codes initializes spark. This installs Apache Spark 3.0.0, Java 8, and Findspark, a library that makes it easy for Python to find Spark. You might also need to refer to correct version while installing. Here I am using 2.4.5. I refered to a medium article by Asif to install spark.\n",
        "\n",
        "> [Link to document guide](https://towardsdatascience.com/pyspark-in-google-colab-6821c2faf41c)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anLHk85lyQ5G"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzXlKXnGysC6"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ6HCddh0rBI"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# sqlContext = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77fvmo9F0szJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "dfd4e7ad-649d-4ab3-a6c9-5f2a38f08a1f"
      },
      "source": [
        "df = spark.sql('''select 'spark' as hello''')\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|spark|\n",
            "+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOt5nGW15fKi"
      },
      "source": [
        "## **Ways to create dataframe**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnkg7pMe5tSy"
      },
      "source": [
        "If you are able to run the codes above, you are good to proceed. Let's start with ways to create dataframe in spark. We will also do some basic data manipulation. But let's start with importing useful libraries first. Then we will look at some useful dataframe methods.\n",
        "<br>\n",
        "<br>\n",
        "Please note that there are many functions available in `pyspark.sql.function` which will be helpful for data analysis and descriptive analytics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWbHuaova0PH"
      },
      "source": [
        "# Import spark libraries\n",
        "from pyspark.sql import Row, DataFrame\n",
        "from pyspark.sql.types import StringType, StructType, StructField, IntegerType\n",
        "from pyspark.sql.functions import col, expr, lit, substring, concat, concat_ws, when, coalesce\n",
        "from pyspark.sql import functions as F # for more such functions\n",
        "from functools import reduce"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAAJP0bypn5z"
      },
      "source": [
        "### View Spark dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8dhv4o6pyah"
      },
      "source": [
        "We use show or collect method as shown below to view dataframe in spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiceReEhpm7i"
      },
      "source": [
        "df.show()\n",
        "OR \n",
        "df.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg0oIX8viJqX"
      },
      "source": [
        "### Create Dataframe from list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN9FFp5UC_KD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "40f5d098-c08d-4c63-8304-bcd069825b73"
      },
      "source": [
        "df = spark.createDataFrame(\n",
        "    [\n",
        "      [\"2015-06-23\", 5],\n",
        "      [\"2016-07-20\", 7]\n",
        "    ], # Data rows\n",
        "    [\"data_date\", \"months_to_add\"] # Column names\n",
        ")\n",
        "\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------------+\n",
            "| data_date|months_to_add|\n",
            "+----------+-------------+\n",
            "|2015-06-23|            5|\n",
            "|2016-07-20|            7|\n",
            "+----------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NekhUJ6rqUYv"
      },
      "source": [
        "### Create Dataframe from RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH_VGsSrqaSt"
      },
      "source": [
        "l =  [[\"2015-06-23\", 5]\n",
        "      ,[\"2016-07-20\", 7]] #List with data elements\n",
        "rdd1 = spark.sparkContext.parallelize(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1pxB6-25YqL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "8c3d1e7f-0f1b-412e-ce42-cee094fa0469"
      },
      "source": [
        "\n",
        "row_rdd = rdd1.map(lambda x: Row(x[0], x[1]))\n",
        "df = spark.createDataFrame(row_rdd, ['data_date', 'months_to_add'])\n",
        "\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------------+\n",
            "| data_date|months_to_add|\n",
            "+----------+-------------+\n",
            "|2015-06-23|            5|\n",
            "|2016-07-20|            7|\n",
            "+----------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9coMPnXJpV3A"
      },
      "source": [
        "### Create Dataframe from RDD and datatype"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv2gbDKPpSms",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "b6426ae3-9cfe-48cf-80ef-35f51ac59caf"
      },
      "source": [
        "\n",
        "schema = StructType([\n",
        "    StructField(\"data_date\", StringType(), True),\n",
        "    StructField(\"months_to_add\", IntegerType(), True)]) # Col, Type, Nullable\n",
        "\n",
        "df = spark.createDataFrame(rdd1, schema)\n",
        "df.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------------+\n",
            "| data_date|months_to_add|\n",
            "+----------+-------------+\n",
            "|2015-06-23|            5|\n",
            "|2016-07-20|            7|\n",
            "+----------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUIRltIfaoQX"
      },
      "source": [
        "### Create Dataframe from lists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdm21E_naono",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "628b4528-aa47-4e24-ba01-72ca76f2edbb"
      },
      "source": [
        "# Building a simple dataframe:\n",
        "schema = StructType([\n",
        "    StructField(\"data_date\", StringType(), True),\n",
        "    StructField(\"months_to_add\", IntegerType(), True)\n",
        "    ]) # Col, Type, Nullable\n",
        "\n",
        "\n",
        "column1 = [\"2015-06-23\", \"2016-07-20\"]\n",
        "column2 = [5, 7]\n",
        "\n",
        "# Dataframe:\n",
        "df = spark.createDataFrame(list(zip(column1, column2)), schema=schema)\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------------+\n",
            "| data_date|months_to_add|\n",
            "+----------+-------------+\n",
            "|2015-06-23|            5|\n",
            "|2016-07-20|            7|\n",
            "+----------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cys0Yf8Cq8cC"
      },
      "source": [
        "### Create Dataframe from Pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUaf3spiqnUc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "1301eebc-3e80-4a9a-e27f-c9de7706792b"
      },
      "source": [
        "import pandas as pd\n",
        "# df = spark.createDataFrame(pandas_df.toPandas()) # Creating pandas dataframe first\n",
        "\n",
        "l =  [[\"2015-06-23\", 5]\n",
        "      ,[\"2016-07-20\", 7]] #List with data elements\n",
        "    \n",
        "df = spark.createDataFrame(pd.DataFrame(l),['data_date','months_to_add'])\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------------+\n",
            "| data_date|months_to_add|\n",
            "+----------+-------------+\n",
            "|2015-06-23|            5|\n",
            "|2016-07-20|            7|\n",
            "+----------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg2Oq8OvQdrQ"
      },
      "source": [
        "### Create Dataframe from hive table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L58WM00TMUO"
      },
      "source": [
        "input_table = <db_name>.<table_name>\n",
        "df = spark.sql('''select data_date, months_to_add from {0}'''.format(input_table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA20eldd5YtW"
      },
      "source": [
        "### Create Dataframe from CSV or other text file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5I-JFQZkiHY"
      },
      "source": [
        "Data is available from data.gov which is FDIC failed bank list. You may download the same from link given or use any text file you have. Data link: https://catalog.data.gov/dataset/fdic-failed-bank-list.\n",
        "<br>\n",
        "<br>\n",
        "Do note the arguments in read function. `header` is True for providing data with first line as header. `inferschema` is just a lazy way of using best possible data types. `delimiter` could be changed to tab (\\t), or space(\\\\s) depending on input file.\n",
        "<br>\n",
        "<br>\n",
        "I have also uploaded the same file to github. Feel free to use the link directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qn3A88f5YoY",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "6230475b-eb3d-40a7-aca5-96f1e6e805a9"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-87f80d18-858f-4347-b8be-5a382bc70f5c\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-87f80d18-858f-4347-b8be-5a382bc70f5c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dsddm7AE246a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "cea26f1d-c332-4aa4-df61-a0c2ca3d4ef5"
      },
      "source": [
        "! ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "banklist.csv  spark-2.4.5-bin-hadoop2.7      spark-2.4.5-bin-hadoop2.7.tgz.1\n",
            "sample_data   spark-2.4.5-bin-hadoop2.7.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5j8RecENZZD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "7616bcc7-4c8f-43cc-b952-fcf81c478684"
      },
      "source": [
        "# inferschema loads the closest datatype automatically from the data\n",
        "# header option reads first line as columns, else default value\n",
        "\n",
        "df = spark.read.options(header=\"true\", inferschema = \"true\", delimiter=\",\").csv('banklist.csv')\n",
        "\n",
        "print('df.count  :', df.count())\n",
        "print('df.col ct :', len(df.columns))\n",
        "print('df.columns:', df.columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df.count  : 561\n",
            "df.col ct : 6\n",
            "df.columns: ['Bank Name', 'City', 'ST', 'CERT', 'Acquiring Institution', 'Closing Date']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMWY41ljNU1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "d20c04b8-24f1-4ad7-8ab7-26c83d7b1208"
      },
      "source": [
        "# Using file uploaded to github\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/akash865/spark_101/master/banklist.csv\"\n",
        "from pyspark import SparkFiles\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "df = spark.read.options(header=\"true\", inferschema = \"true\", delimiter=\",\").csv(\"file://\"+SparkFiles.get(\"banklist.csv\"))\n",
        "\n",
        "print('df.count  :', df.count())\n",
        "print('df.col ct :', len(df.columns))\n",
        "print('df.columns:', df.columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df.count  : 561\n",
            "df.col ct : 6\n",
            "df.columns: ['Bank Name', 'City', 'ST', 'CERT', 'Acquiring Institution', 'Closing Date']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lviT9J99TMmr"
      },
      "source": [
        "## **Using SQL in spark**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l11ahwlql8yM"
      },
      "source": [
        "Often there is a need to use SQL for data manipulation. Though spark provides nearly all functions that we use in SQL, it is often easier to use SQL because of its familiarity. Please do note that it is best practice to avoid spaces in column names (in the example below). We will also learn spark functions/methods that will help as we go along. We can start with schema details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jce3V6YyT_tm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "d0572acf-eaba-4375-aa16-76b335ce4839"
      },
      "source": [
        "df.registerTempTable(\"temp_tb\")\n",
        "\n",
        "df_check = spark.sql('''select `Bank Name`, City, `Closing Date` from temp_tb''')\n",
        "df_check.show(4, truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------+-------------+------------+\n",
            "|Bank Name                       |City         |Closing Date|\n",
            "+--------------------------------+-------------+------------+\n",
            "|The First State Bank            |Barboursville|3-Apr-20    |\n",
            "|Ericson State Bank              |Ericson      |14-Feb-20   |\n",
            "|City National Bank of New Jersey|Newark       |1-Nov-19    |\n",
            "|Resolute Bank                   |Maumee       |25-Oct-19   |\n",
            "+--------------------------------+-------------+------------+\n",
            "only showing top 4 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_h4nezoVoMP"
      },
      "source": [
        "## **Dataframe Basic Operations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLoqnwVgtucp"
      },
      "source": [
        "### Describe dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9DfA6Lnt5gC"
      },
      "source": [
        "Describe is a useful method which performs `count`, `mean`, `stddev`, `min` and `max` on all columns. We could limit our variables by passing in column name(s) in `describe`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJSOwS4Eturw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "c313f867-7b89-42f5-ac96-a28a6e169dc4"
      },
      "source": [
        "df.describe().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+--------------------+-------+----+-----------------+---------------------+------------+\n",
            "|summary|           Bank Name|   City|  ST|             CERT|Acquiring Institution|Closing Date|\n",
            "+-------+--------------------+-------+----+-----------------+---------------------+------------+\n",
            "|  count|                 561|    561| 561|              561|                  561|         561|\n",
            "|   mean|                null|   null|null|31685.68449197861|                 null|        null|\n",
            "| stddev|                null|   null|null|16446.65659309965|                 null|        null|\n",
            "|    min|1st American Stat...|Acworth|  AL|               91|      1st United Bank|    1-Aug-08|\n",
            "|    max|               ebank|Wyoming|  WY|            58701|  Your Community Bank|    9-Sep-11|\n",
            "+-------+--------------------+-------+----+-----------------+---------------------+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwVXN5fZn3Jz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "7f5019f8-8df1-4f25-a700-14511a2c3ece"
      },
      "source": [
        "df.describe('City', 'ST').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-------+----+\n",
            "|summary|   City|  ST|\n",
            "+-------+-------+----+\n",
            "|  count|    561| 561|\n",
            "|   mean|   null|null|\n",
            "| stddev|   null|null|\n",
            "|    min|Acworth|  AL|\n",
            "|    max|Wyoming|  WY|\n",
            "+-------+-------+----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDht33CnhVL5"
      },
      "source": [
        "### Counts, Columns and Schema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O63DzXi1VoaN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "939c5a84-dae4-45c4-b18d-5b73dacfc308"
      },
      "source": [
        "print('df.count\t\t:', df.count())\n",
        "print('df.columns\t:', df.columns)\n",
        "print('df dtypes\t:', df.dtypes)\n",
        "print('df schema 1:', df.schema)\n",
        "print('df schema 2:', df.printSchema())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df.count\t\t: 561\n",
            "df.colums\t: ['Bank Name', 'City', 'ST', 'CERT', 'Acquiring Institution', 'Closing Date']\n",
            "df dtypes\t: [('Bank Name', 'string'), ('City', 'string'), ('ST', 'string'), ('CERT', 'int'), ('Acquiring Institution', 'string'), ('Closing Date', 'string')]\n",
            "df schema 1: StructType(List(StructField(Bank Name,StringType,true),StructField(City,StringType,true),StructField(ST,StringType,true),StructField(CERT,IntegerType,true),StructField(Acquiring Institution,StringType,true),StructField(Closing Date,StringType,true)))\n",
            "root\n",
            " |-- Bank Name: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- ST: string (nullable = true)\n",
            " |-- CERT: integer (nullable = true)\n",
            " |-- Acquiring Institution: string (nullable = true)\n",
            " |-- Closing Date: string (nullable = true)\n",
            "\n",
            "df schema 2: None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSHkkGVBhd-F"
      },
      "source": [
        "### Remove Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcqzJ65IV0Fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b91fc2d5-66c7-4a3c-e466-7eb1cccb9dbe"
      },
      "source": [
        "df = df.dropDuplicates()\n",
        "print('df.count\t\t:', df.count())\n",
        "print('df.columns\t:', df.columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df.count\t\t: 561\n",
            "df.colums\t: ['Bank Name', 'City', 'ST', 'CERT', 'Acquiring Institution', 'Closing Date']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olIfm2KIicOn"
      },
      "source": [
        "### Select specific columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wBMsReAWSUF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "80805270-5c24-4d08-ff08-b8d474d3908e"
      },
      "source": [
        "df2 = df.select(*['Bank Name', 'City'])\n",
        "df2.show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+\n",
            "|           Bank Name|         City|\n",
            "+--------------------+-------------+\n",
            "|The First State Bank|Barboursville|\n",
            "|  Ericson State Bank|      Ericson|\n",
            "+--------------------+-------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMvFVdP_oK7W"
      },
      "source": [
        "### Select multiple columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaiTNIJhotbA"
      },
      "source": [
        "Alternatively we could remove multiple columns from dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS3tpP6qoLVu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "8a22089b-14a7-49fc-84a1-233e478482f6"
      },
      "source": [
        "col_l = list(set(df.columns)  - {'CERT','ST'})\n",
        "df2 = df.select(*col_l)\n",
        "df2.show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+---------------------+------------+-------------+\n",
            "|           Bank Name|Acquiring Institution|Closing Date|         City|\n",
            "+--------------------+---------------------+------------+-------------+\n",
            "|The First State Bank|       MVB Bank, Inc.|    3-Apr-20|Barboursville|\n",
            "|  Ericson State Bank| Farmers and Merch...|   14-Feb-20|      Ericson|\n",
            "+--------------------+---------------------+------------+-------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zfc5uWaoi_R2"
      },
      "source": [
        "### Rename columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opk9G1Xyju0e"
      },
      "source": [
        "We will rename multiple columns in the example below. Let's remove spaces and make columns more understable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdOkLYzlisvy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "b6adb081-b778-4c8b-fdb7-ae18afc6e4d4"
      },
      "source": [
        "df2 = df \\\n",
        "  .withColumnRenamed('Bank Name'            , 'bank_name') \\\n",
        "  .withColumnRenamed('Acquiring Institution', 'acq_institution') \\\n",
        "  .withColumnRenamed('Closing Date'         , 'closing_date') \\\n",
        "  .withColumnRenamed('ST'                   , 'state') \\\n",
        "  .withColumnRenamed('CERT'                 , 'cert') #\\\n",
        "\n",
        "df2.show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+-----+-----+---------------+------------+\n",
            "|           bank_name|    City|state| cert|acq_institution|closing_date|\n",
            "+--------------------+--------+-----+-----+---------------+------------+\n",
            "| First Bank of Idaho| Ketchum|   ID|34396|U.S. Bank, N.A.|   24-Apr-09|\n",
            "|Amcore Bank, Nati...|Rockford|   IL| 3735|    Harris N.A.|   23-Apr-10|\n",
            "+--------------------+--------+-----+-----+---------------+------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veAUD18tl6oG"
      },
      "source": [
        "### Rename columns using loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZaQTHwOXGo5"
      },
      "source": [
        "If we have multiple columns, we could use loop to help us perform similar operations like case change, replace characters. In the below example, we are replacing all spaces with `_`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gnKDBPjjpka",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "d34619a3-86df-4ace-8ca1-3d9ae0683a26"
      },
      "source": [
        "rename_expr = [col(column).alias(column.replace(' ', '_')) for column in df.columns]\n",
        "\n",
        "df2 = df.select(*rename_expr)\n",
        "df2.show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+---+-----+---------------------+------------+\n",
            "|           Bank_Name|         City| ST| CERT|Acquiring_Institution|Closing_Date|\n",
            "+--------------------+-------------+---+-----+---------------------+------------+\n",
            "|The First State Bank|Barboursville| WV|14361|       MVB Bank, Inc.|    3-Apr-20|\n",
            "|  Ericson State Bank|      Ericson| NE|18265| Farmers and Merch...|   14-Feb-20|\n",
            "+--------------------+-------------+---+-----+---------------------+------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "momxuupJnqPd"
      },
      "source": [
        "### Add columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L-DTkoAn_KP"
      },
      "source": [
        "In the below example, we will copy the column state using `col` function. PySpark SQL libraries contains many functions which we will be using through. Note that `col` is one such function which returns column values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDKwdHg5l_nm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "e8d1e3d9-e37b-4e9e-c63f-7fff51140057"
      },
      "source": [
        "df2 = df.withColumn('state', col('ST'))\n",
        "df2.show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+---+-----+---------------------+------------+-----+\n",
            "|           Bank Name|    City| ST| CERT|Acquiring Institution|Closing Date|state|\n",
            "+--------------------+--------+---+-----+---------------------+------------+-----+\n",
            "| First Bank of Idaho| Ketchum| ID|34396|      U.S. Bank, N.A.|   24-Apr-09|   ID|\n",
            "|Amcore Bank, Nati...|Rockford| IL| 3735|          Harris N.A.|   23-Apr-10|   IL|\n",
            "+--------------------+--------+---+-----+---------------------+------------+-----+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjkXUUpvou7B"
      },
      "source": [
        "### Add constant column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOyNmNHUo8s0"
      },
      "source": [
        "We will be using `lit` function to add a constant value. `lit` is an acronym for linear transform which transforms a single value to multiple rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-gSou1GnpBd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "4192088c-1f2b-43be-9f79-22451c7dfe95"
      },
      "source": [
        "df2 = df.withColumn('country', lit('US'))\n",
        "df2.show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+---+-----+---------------------+------------+-------+\n",
            "|           Bank Name|    City| ST| CERT|Acquiring Institution|Closing Date|country|\n",
            "+--------------------+--------+---+-----+---------------------+------------+-------+\n",
            "| First Bank of Idaho| Ketchum| ID|34396|      U.S. Bank, N.A.|   24-Apr-09|     US|\n",
            "|Amcore Bank, Nati...|Rockford| IL| 3735|          Harris N.A.|   23-Apr-10|     US|\n",
            "+--------------------+--------+---+-----+---------------------+------------+-------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx7fAtmqZxSH"
      },
      "source": [
        "### Drop columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAKuWe7nZxmc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "fb8198ff-f692-408d-b82f-6ba951b6a041"
      },
      "source": [
        "df2 = df.drop('CERT')\n",
        "df2.show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+---+---------------------+------------+\n",
            "|           Bank Name|         City| ST|Acquiring Institution|Closing Date|\n",
            "+--------------------+-------------+---+---------------------+------------+\n",
            "|The First State Bank|Barboursville| WV|       MVB Bank, Inc.|    3-Apr-20|\n",
            "|  Ericson State Bank|      Ericson| NE| Farmers and Merch...|   14-Feb-20|\n",
            "+--------------------+-------------+---+---------------------+------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG7tuEZpaGll"
      },
      "source": [
        "### Drop multiple columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGLxvJ1baUww"
      },
      "source": [
        "It's also simple. All we have to do is pass a list of columns using `*` to remove. Below are 2 ways to doing the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56E1uJPQpZJH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "d418390e-d93a-4fa4-85a4-7e67a2ddca81"
      },
      "source": [
        "df2 = df.drop(*['CERT','ST'])\n",
        "df2.show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+---------------------+------------+\n",
            "|           Bank Name|         City|Acquiring Institution|Closing Date|\n",
            "+--------------------+-------------+---------------------+------------+\n",
            "|The First State Bank|Barboursville|       MVB Bank, Inc.|    3-Apr-20|\n",
            "|  Ericson State Bank|      Ericson| Farmers and Merch...|   14-Feb-20|\n",
            "+--------------------+-------------+---------------------+------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuVEE3SqaFvC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "abf19f5c-6bfc-46b7-ebb7-f61131f7920c"
      },
      "source": [
        "df2 = reduce(DataFrame.drop, ['CERT','ST'], df)\n",
        "df2.show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+---------------------+------------+\n",
            "|           Bank Name|         City|Acquiring Institution|Closing Date|\n",
            "+--------------------+-------------+---------------------+------------+\n",
            "|The First State Bank|Barboursville|       MVB Bank, Inc.|    3-Apr-20|\n",
            "|  Ericson State Bank|      Ericson| Farmers and Merch...|   14-Feb-20|\n",
            "+--------------------+-------------+---------------------+------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhuwX3AwbRiu"
      },
      "source": [
        "### Filter data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r68C7hXnpzwq"
      },
      "source": [
        "Below we have examples to filter data using `>`, `<`, `==`, `between` and `isin`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeA5BOpJbD5O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "3b777e82-b6b4-4e44-d36a-12bdb28c5492"
      },
      "source": [
        "# Equal to values\n",
        "df2 = df.where(df['ST'] == 'NE')\n",
        "\n",
        "# Between values\n",
        "df3 = df.where(df['CERT'].between('1000','2000'))\n",
        "\n",
        "# Is inside multiple values\n",
        "df4 = df.where(df['ST'].isin('NE','IL'))\n",
        "\n",
        "print('df.count  :', df.count())\n",
        "print('df2.count :', df2.count())\n",
        "print('df3.count :', df3.count())\n",
        "print('df4.count :', df4.count())\n",
        "\n",
        "print('\\ndf2 sample below')\n",
        "df2.show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df.count  : 561\n",
            "df2.count : 4\n",
            "df3.count : 9\n",
            "df4.count : 73\n",
            "\n",
            "df2 sample below\n",
            "+-------------------+-------+---+-----+---------------------+------------+\n",
            "|          Bank Name|   City| ST| CERT|Acquiring Institution|Closing Date|\n",
            "+-------------------+-------+---+-----+---------------------+------------+\n",
            "| Ericson State Bank|Ericson| NE|18265| Farmers and Merch...|   14-Feb-20|\n",
            "|Mid City Bank, Inc.|  Omaha| NE|19397|         Premier Bank|    4-Nov-11|\n",
            "+-------------------+-------+---+-----+---------------------+------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvyiT6Virnk0"
      },
      "source": [
        "### Filter data using logical operators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9vc-eJgrs8r"
      },
      "source": [
        "If we need to filter using multiple conditions, we could use logical operators. Please note the logical operators here, `AND`:`&`, `OR`:`|` and `NOT`:`!`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2jx8gV3q70-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "13c4f57c-30a3-45b0-d295-ddc2b826b38c"
      },
      "source": [
        "df2 = df.where((df['ST'] == 'NE') & (df['City'] == 'Ericson'))\n",
        "df2.show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+-------+---+-----+---------------------+------------+\n",
            "|         Bank Name|   City| ST| CERT|Acquiring Institution|Closing Date|\n",
            "+------------------+-------+---+-----+---------------------+------------+\n",
            "|Ericson State Bank|Ericson| NE|18265| Farmers and Merch...|   14-Feb-20|\n",
            "+------------------+-------+---+-----+---------------------+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bImSbNBFcsNl"
      },
      "source": [
        "### Cast datatypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX7goybEcsNn"
      },
      "source": [
        "Often we would need to change the datatype for a particular variable. When we create dataframe using text files, there is a possibility thaof datatype mismatch, due to some errors/missing in data. We could cast this to a different datatype as needed. We will deal with datetypes later on. Please note the two methods to cast datatype as string below. We could also use same variable name in the tranformed variable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kUFFW4FcsNp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "a7e97602-6a6c-42c4-fcbb-9042ff6444c2"
      },
      "source": [
        "print(df.printSchema())\n",
        "\n",
        "df2 = df \\\n",
        ".withColumn('CERT_str1', df['CERT'].cast('string')) \\\n",
        ".withColumn('CERT_str2', df['CERT'].cast(StringType())) #\\\n",
        "\n",
        "print('Post cast')\n",
        "print(df2.printSchema())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Bank Name: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- ST: string (nullable = true)\n",
            " |-- CERT: integer (nullable = true)\n",
            " |-- Acquiring Institution: string (nullable = true)\n",
            " |-- Closing Date: string (nullable = true)\n",
            "\n",
            "None\n",
            "Post cast\n",
            "root\n",
            " |-- Bank Name: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- ST: string (nullable = true)\n",
            " |-- CERT: integer (nullable = true)\n",
            " |-- Acquiring Institution: string (nullable = true)\n",
            " |-- Closing Date: string (nullable = true)\n",
            " |-- CERT_str1: string (nullable = true)\n",
            " |-- CERT_str2: string (nullable = true)\n",
            "\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj8Vbx0gYqQ-"
      },
      "source": [
        "### Coalesce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCznZIdnYqN3"
      },
      "source": [
        "Coalesce is a word which I frequently started using after learning SQL. In some cases, we are required to replace some columns containing nulls by a constant or maybe another column in the same data. We could use `CASE` statements to priortize replacing NULLs or using `coalesce` comes in handy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ-Z7Z0FYsRS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "d7d15e05-7213-4bf8-8933-c82d411d267d"
      },
      "source": [
        "df = spark.createDataFrame(\n",
        "    [\n",
        "      ['a1', 5, 5],\n",
        "      ['a2', 7, 11],\n",
        "      ['a3', None, 10],\n",
        "      ['a4', 10, 15]\n",
        "    ], # Data rows\n",
        "    ['user', 'day_1', 'day_2'] # Column names\n",
        ")\n",
        "\n",
        "df.show()\n",
        "\n",
        "df2 = df.withColumn('day', coalesce(col('day_1'), col('day_2'), lit(0)))\n",
        "df2.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+-----+\n",
            "|user|day_1|day_2|\n",
            "+----+-----+-----+\n",
            "|  a1|    5|    5|\n",
            "|  a2|    7|   11|\n",
            "|  a3| null|   10|\n",
            "|  a4|   10|   15|\n",
            "+----+-----+-----+\n",
            "\n",
            "+----+-----+-----+---+\n",
            "|user|day_1|day_2|day|\n",
            "+----+-----+-----+---+\n",
            "|  a1|    5|    5|  5|\n",
            "|  a2|    7|   11|  7|\n",
            "|  a3| null|   10| 10|\n",
            "|  a4|   10|   15| 10|\n",
            "+----+-----+-----+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOAWTJcIVLrs"
      },
      "source": [
        "### Replace values in dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2hmf7qfVLpS"
      },
      "source": [
        "Replacing all values in dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJkLrgi-VMqM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "25be16f8-8f64-45c6-b40f-ff1ee2580d11"
      },
      "source": [
        "# Pre replace\n",
        "df.show(2)\n",
        "# Post replace\n",
        "df.na.replace(7,17).show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+-----+\n",
            "|user|day_1|day_2|\n",
            "+----+-----+-----+\n",
            "|  a1|    5|    5|\n",
            "|  a2|    7|   11|\n",
            "+----+-----+-----+\n",
            "only showing top 2 rows\n",
            "\n",
            "+----+-----+-----+\n",
            "|user|day_1|day_2|\n",
            "+----+-----+-----+\n",
            "|  a1|    5|    5|\n",
            "|  a2|   17|   11|\n",
            "+----+-----+-----+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "658R7Nz6Xpa_"
      },
      "source": [
        "### Sort values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hNCHNRvXpVN"
      },
      "source": [
        "Sorting in dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD0bV-nAXse2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "2436d0d5-965c-4d55-cbee-0c17af0bdef7"
      },
      "source": [
        "# Default - ascending\n",
        "df.sort('day_1').show(2)\n",
        "\n",
        "# Descending sort\n",
        "df.sort(col('day_1').desc()).show(2)\n",
        "## ANOTHER WAY TO WRITE THE STATEMENT IS ==> df.sort('ST',ascending=False).show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+-----+\n",
            "|user|day_1|day_2|\n",
            "+----+-----+-----+\n",
            "|  a3| null|   10|\n",
            "|  a1|    5|    5|\n",
            "+----+-----+-----+\n",
            "only showing top 2 rows\n",
            "\n",
            "+----+-----+-----+\n",
            "|user|day_1|day_2|\n",
            "+----+-----+-----+\n",
            "|  a4|   10|   15|\n",
            "|  a2|    7|   11|\n",
            "+----+-----+-----+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgW41FENuWRg"
      },
      "source": [
        "# Spark 102 - Some additional methods/functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtk4XpZ7ulXa"
      },
      "source": [
        "Now that we know some basic spark operations from previous section which will be used very frequently. We can move on to look at additional functions which are also helpful. In this section we will be using functions from `pyspark.sql.functions` very often. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8vtt3LEvg7t"
      },
      "source": [
        "### String functions - concat, concat_ws and substring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXvW9snDRq7B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "0954edd0-701e-4430-ba69-7635101bafcf"
      },
      "source": [
        "# Input the file \n",
        "# Using file uploaded to github\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/akash865/spark_101/master/banklist.csv\"\n",
        "from pyspark import SparkFiles\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "df = spark.read.options(header=\"true\", inferschema = \"true\", delimiter=\",\").csv(\"file://\"+SparkFiles.get(\"banklist.csv\"))\n",
        "\n",
        "print('df.count  :', df.count())\n",
        "print('df.col ct :', len(df.columns))\n",
        "print('df.columns:', df.columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df.count  : 561\n",
            "df.col ct : 6\n",
            "df.columns: ['Bank Name', 'City', 'ST', 'CERT', 'Acquiring Institution', 'Closing Date']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysj_eHSfuZEM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "d6a0ca27-2804-47a5-dc36-c21925b08e2a"
      },
      "source": [
        "# Substring - Substring starts from the specified position to the specified length\n",
        "df2 = df.withColumn('city_index', substring(col('ST'),1,1))\n",
        "df2.show(2)\n",
        "\n",
        "# Concat - Concatenates multiple input string columns together into a single string column\n",
        "df2 = df.withColumn('location', concat('City','ST'))\n",
        "df2.show(2)\n",
        "\n",
        "# Concat ws - Concatenates multiple input string columns together with the specified seperator into a single string column\n",
        "df2 = df.withColumn('location', concat_ws('-','City','ST'))\n",
        "df2.show(2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+---+-----+---------------------+------------+----------+\n",
            "|           Bank Name|         City| ST| CERT|Acquiring Institution|Closing Date|city_index|\n",
            "+--------------------+-------------+---+-----+---------------------+------------+----------+\n",
            "|The First State Bank|Barboursville| WV|14361|       MVB Bank, Inc.|    3-Apr-20|         W|\n",
            "|  Ericson State Bank|      Ericson| NE|18265| Farmers and Merch...|   14-Feb-20|         N|\n",
            "+--------------------+-------------+---+-----+---------------------+------------+----------+\n",
            "only showing top 2 rows\n",
            "\n",
            "+--------------------+-------------+---+-----+---------------------+------------+---------------+\n",
            "|           Bank Name|         City| ST| CERT|Acquiring Institution|Closing Date|       location|\n",
            "+--------------------+-------------+---+-----+---------------------+------------+---------------+\n",
            "|The First State Bank|Barboursville| WV|14361|       MVB Bank, Inc.|    3-Apr-20|BarboursvilleWV|\n",
            "|  Ericson State Bank|      Ericson| NE|18265| Farmers and Merch...|   14-Feb-20|      EricsonNE|\n",
            "+--------------------+-------------+---+-----+---------------------+------------+---------------+\n",
            "only showing top 2 rows\n",
            "\n",
            "+--------------------+-------------+---+-----+---------------------+------------+----------------+\n",
            "|           Bank Name|         City| ST| CERT|Acquiring Institution|Closing Date|        location|\n",
            "+--------------------+-------------+---+-----+---------------------+------------+----------------+\n",
            "|The First State Bank|Barboursville| WV|14361|       MVB Bank, Inc.|    3-Apr-20|Barboursville-WV|\n",
            "|  Ericson State Bank|      Ericson| NE|18265| Farmers and Merch...|   14-Feb-20|      Ericson-NE|\n",
            "+--------------------+-------------+---+-----+---------------------+------------+----------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MA7gE1PyAsf"
      },
      "source": [
        "### Cross tab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1O7Rp_HyUwI"
      },
      "source": [
        "Cross tabulation provides frequency distribution between set of variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suyOfZRVwMj3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "e411f793-a36f-40bc-a040-d2eb550614ba"
      },
      "source": [
        "df2 = df.stat.crosstab('City', 'ST')\n",
        "df2.show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "|       City_ST| AL| AR| AZ| CA| CO| CT| FL| GA| HI| IA| ID| IL| IN| KS| KY| LA| MA| MD| MI| MN| MO| MS| NC| NE| NH| NJ| NM| NV| NY| OH| OK| OR| PA| PR| SC| SD| TN| TX| UT| VA| WA| WI| WV| WY|\n",
            "+--------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "|       Clayton|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
            "|Salt Lake City|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|\n",
            "+--------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi2DunYPyiK9"
      },
      "source": [
        "### Case Statements OR if/else"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XKRdjePmLPj"
      },
      "source": [
        "A simple case statements could be written using `when` or `expr` from `pyspark.sql` library. We could also use UDF (we will see examples later) to do the same. \n",
        "<br>\n",
        "<br>\n",
        "Using `when` gets confusing sometimes with more elif statements. I prefer to use `when` in case of a simple if/else condition. For criteria with multiple elif statements, its convenient to use `expr`. Please see use of operators `=` and `like` below with wildcard `%`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhPqFZY1yNSX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "e58df463-24a3-4996-af09-228b22cc3c4a"
      },
      "source": [
        "df2 = df.withColumn('state_ne', when(col('ST')=='NE', 1).otherwise(0))\n",
        "df2.show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+---+-----+---------------------+------------+--------+\n",
            "|           Bank Name|         City| ST| CERT|Acquiring Institution|Closing Date|state_ne|\n",
            "+--------------------+-------------+---+-----+---------------------+------------+--------+\n",
            "|The First State Bank|Barboursville| WV|14361|       MVB Bank, Inc.|    3-Apr-20|       0|\n",
            "|  Ericson State Bank|      Ericson| NE|18265| Farmers and Merch...|   14-Feb-20|       1|\n",
            "+--------------------+-------------+---+-----+---------------------+------------+--------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBgxHLPmnVBS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "0229dcbc-9a1a-4c1a-b168-ef4cf3e9b585"
      },
      "source": [
        "state_val = expr(\"\"\"\n",
        "  IF(ST is NULL, NULL\n",
        "  ,IF(ST = 'NE', 1\n",
        "  ,IF(ST = 'WV', 2\n",
        "  , 3\n",
        "  )))\n",
        "\"\"\")\n",
        "\n",
        "df2 = df.withColumn('state_val', state_val)\n",
        "df2.show(3)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+---+-----+---------------------+------------+---------+\n",
            "|           Bank Name|         City| ST| CERT|Acquiring Institution|Closing Date|state_val|\n",
            "+--------------------+-------------+---+-----+---------------------+------------+---------+\n",
            "|The First State Bank|Barboursville| WV|14361|       MVB Bank, Inc.|    3-Apr-20|        2|\n",
            "|  Ericson State Bank|      Ericson| NE|18265| Farmers and Merch...|   14-Feb-20|        1|\n",
            "|City National Ban...|       Newark| NJ|21111|      Industrial Bank|    1-Nov-19|        3|\n",
            "+--------------------+-------------+---+-----+---------------------+------------+---------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8TnUifcpvLL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "84050edc-f17a-4c24-c6dc-2ca23da8e545"
      },
      "source": [
        "bank_val = expr(\"\"\"\n",
        "  IF(City is NULL, NULL\n",
        "  ,IF(City like '%ville%', 'Ville'\n",
        "  , 'Other'\n",
        "  ))\n",
        "\"\"\")\n",
        "\n",
        "df2 = df.withColumn('bank_val', bank_val)\n",
        "df2.show(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+---+-----+---------------------+------------+--------+\n",
            "|           Bank Name|         City| ST| CERT|Acquiring Institution|Closing Date|bank_val|\n",
            "+--------------------+-------------+---+-----+---------------------+------------+--------+\n",
            "|The First State Bank|Barboursville| WV|14361|       MVB Bank, Inc.|    3-Apr-20|   Ville|\n",
            "|  Ericson State Bank|      Ericson| NE|18265| Farmers and Merch...|   14-Feb-20|   Other|\n",
            "+--------------------+-------------+---+-----+---------------------+------------+--------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6jCslcaQdrI"
      },
      "source": [
        "### NULL values treatment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WefTPbLvQtjN"
      },
      "source": [
        "Null values are part of data as most of the data we encounter are not clean. A step in cleaning variables include checking for nulls before using any data.\n",
        "<br>\n",
        "<br>\n",
        "The null values in python have a datatype called `NoneType` therefore to check for null values we must always use `isNone()` and `isnotNone()` functions for columns. Any columns having null values can't be passed as an input to any pyspark function because the functions can't take Null values. So we must first check the null values using `isNull` function of the dataframe and should treat them first .\n",
        "<br>\n",
        "<br>\n",
        "The bank list that we are using doesn't contain any null values, but the methods that we're using below are most commonly used to clean our data.\n",
        "1. Filter nulls using `where`\n",
        "2. Checking nulls across all columns using `for` loop\n",
        "3. Replacing null with contant using `fillna`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQcdo7w1c8_g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "71772ae2-6c44-47fd-8cf7-6c2d0870bf52"
      },
      "source": [
        "df = spark.createDataFrame(\n",
        "    [\n",
        "      ['a1', 5, 5],\n",
        "      ['a2', 7, 11],\n",
        "      ['a3', None, 10],\n",
        "      ['a4', 10, 15]\n",
        "    ], # Data rows\n",
        "    ['user', 'day_1', 'day_2'] # Column names\n",
        ")\n",
        "\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+-----+\n",
            "|user|day_1|day_2|\n",
            "+----+-----+-----+\n",
            "|  a1|    5|    5|\n",
            "|  a2|    7|   11|\n",
            "|  a3| null|   10|\n",
            "|  a4|   10|   15|\n",
            "+----+-----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuvJ5D5koWqW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "0e6be7f6-2613-44e6-c4d8-1aa77290627c"
      },
      "source": [
        "# 1. Checking a column for NULLS\n",
        "df.where(col('day_1').isNull()).count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea4x7XHMR9yq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "b531a0ca-e173-4d39-9807-0f0aafb589a6"
      },
      "source": [
        "# 2. Checking all columns for NULLS using `for` loop\n",
        "for col_i in df.columns:\n",
        "  print('Null count for col', col_i, ':', df.where(col(col_i).isNull()).count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Null count for col user : 0\n",
            "Null count for col day_1 : 1\n",
            "Null count for col day_2 : 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKNfHX4cSfid",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "1288d7a0-5bb2-4026-c9e0-47d3a06be310"
      },
      "source": [
        "# 3. Replacing all NULLS with a constant in a list of columns\n",
        "df = df.fillna(0, subset=['day_1', 'day_2'])\n",
        "\n",
        "# Post replace\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+-----+\n",
            "|user|day_1|day_2|\n",
            "+----+-----+-----+\n",
            "|  a1|    5|    5|\n",
            "|  a2|    7|   11|\n",
            "|  a3|    0|   10|\n",
            "|  a4|   10|   15|\n",
            "+----+-----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI1uhXiLdLRF"
      },
      "source": [
        "### Date formats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmZ7fnPCdLei"
      },
      "source": [
        "This section is going to be a long mostly due to confusions around using date format. Below we will see how we can treat date formats and some addtional methods or functions that are required for date manipulations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZptKiJ6TUw-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "dac900a0-8edd-4768-cb94-82875ed4b43f"
      },
      "source": [
        "# Dataframe with dates\n",
        "df = spark.createDataFrame(\n",
        "    [\n",
        "      [\"2015-09-23\", 5],\n",
        "      [\"2016-07-20\", 7]\n",
        "    ], # Data rows\n",
        "    [\"data_date\", \"months_to_add\"] # Column names\n",
        ")\n",
        "\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------------+\n",
            "| data_date|months_to_add|\n",
            "+----------+-------------+\n",
            "|2015-09-23|            5|\n",
            "|2016-07-20|            7|\n",
            "+----------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siveG2_sqTGS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "2c0979e7-858e-45a9-c75a-dea40aa7d853"
      },
      "source": [
        "df.sort(col('data_date'), ascending=False).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------------+\n",
            "| data_date|months_to_add|\n",
            "+----------+-------------+\n",
            "|2016-07-20|            7|\n",
            "|2015-09-23|            5|\n",
            "+----------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCHdP7uPgfOM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "3610b450-74b5-433e-a28e-c68a478e4ac8"
      },
      "source": [
        "print('df dtypes\t:', df.dtypes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df dtypes\t: [('data_date', 'string'), ('months_to_add', 'bigint')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaZhHpnWhfYi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "e82c12f9-e8e5-4db7-de9d-0ee04627cee4"
      },
      "source": [
        "df.withColumn('a', F.date_add(col('data_date'), 5)).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------------+----------+\n",
            "| data_date|months_to_add|         a|\n",
            "+----------+-------------+----------+\n",
            "|2015-09-23|            5|2015-09-28|\n",
            "|2016-07-20|            7|2016-07-25|\n",
            "+----------+-------------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eavk4KIfuuFe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}